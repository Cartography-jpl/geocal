#! /usr/bin/env python
#
# This sets up everything for using torque to create a DEM

from afids import *
from functools import partial
import subprocess
import os

version = "February 6, 2013"
usage='''Usage: 
  setup_dem_job [options] <igc_collection> <index_image1> 
        <index_image2> <output_base>
  setup_dem_job [options] <igc_collection> <surface_image1>
        <surface_image2> <index_image1> <index_image2> <output_base>
  setup_dem_job -h | --help
  setup_dem_job -v | --version

It currently takes a long time to create a full DEM from a pair of
large images (e.g. the 35,000 x 25,000 typical of WorldView 2). To
get this to run in a reasonable amount of time, we break the job into
a number of tasks which can then be submitted to torque.

This script does this setup. It initializes the database structures for
handling this work, it creates the initial empty DEM, and it creates a
script that can be used to submit the jobs to torque. We don't actually
submit them in this script, but leave this to the user to actually do this.

There are several output files generated from the base name. If the file
base name as "output", then we generate "output.img", "output.job_script",
"output.db", and the directory "output_log". The file "output.img" is the
DEM, it will initially be filled with 0's. The file "output.job_script" is
the script that will submit jobs to torque. The file "output.db" is a scratch
file that contains inputs and the status of the jobs as they are run. The
directory "output_log" will hold the log files from torque as the jobs are
run.

The IGC collection should be specified as a "file:key" pair, and you need
to select a pair of 0-based image index to work on (e.g., 0 and 1 for the
first and second image in the collection).  You can optionally supply
a surface projected version of the images, which should have been performed
using the same IGC collection (e.g., you can run the program igc_project).
This often give better DEM coverage than directly matching the unprojected
images.

Options:
  -h --help         
       Print this message

  --process-number-line=n
       Number of lines to include in each tile that we work on. This 
       controls how long each torque job takes to run. [default: 1000]

  --process-number-sample=n
       Number of lines to include in each tile that we work on. This 
       controls how long each torque job takes to run. [default: 1000]

  --resolution=r
       Resolution in meters of output. [default: 1.0]

  -v --version      
       Print program version
'''
args = docopt_simple(usage, version=version)

igc_col = read_shelve(args.igc_collection)
igc1 = igc_col.image_ground_connection(args.index_image1)
igc2 = igc_col.image_ground_connection(args.index_image2)

# Create the empty DEM

mibase = cib01_mapinfo()
resbase = mibase.resolution_meter
miscale = mibase.scale(args.resolution / resbase,
                       args.resolution / resbase)
midem = igc1.cover(miscale)

demname = args.output_base + ".img"
f = VicarRasterImage(demname, midem, "REAL")
# Set nodata value as metadata. This is the fill value used in height_grid.
f["NODATA"] = -9999.0
# Close and reopen file so we can get offset to where data is.
f.close()
f = VicarLiteFile(demname)
aoi_full = f.map_info
offset = f.data_offset
nline = f.number_line
nsamp = f.number_sample
f = None

# Temporary, also write out filled version
demname_filled = args.output_base + "_filled.img"
f = VicarRasterImage(demname_filled, midem, "REAL")
# Set nodata value as metadata. This is the fill value used in height_grid.
f["NODATA"] = -9999.0
# Close and reopen file so we can get offset to where data is.
f.close()
f = VicarLiteFile(demname_filled)
offset_filled = f.data_offset
f = None

# Set up to read surface data, if we have it.
surface_image1 = None
surface_image2 = None
if(args.surface_image1):
    surface_image1 = read_shelve(args.surface_image1)
    surface_image2 = read_shelve(args.surface_image2)

# Create all the jobs that we will need to run
job_index = 0
database_fname = args.output_base + ".db"
try:
    os.remove(database_fname)
except OSError as exc:
    pass                    # Ok if doesn't exist

for lstart in range(0, nline, args.process_number_line):
    tile_nline = args.process_number_line
    if(lstart + tile_nline > nline):
        tile_nline = nline - lstart
    for sstart in range(0, nsamp, args.process_number_sample):
        tile_nsamp = args.process_number_sample
        if(sstart + tile_nsamp > nsamp):
            tile_nsamp = nsamp - sstart
        dg = DemGenerate(igc1, igc2, 
                      aoi_full.subset(sstart, lstart, tile_nsamp, tile_nline),
                         surface_image1 = surface_image1,
                         surface_image2 = surface_image2)
        job = partial(dem_generate_tile, dg, demname, offset, lstart, sstart,
                      tile_nline, tile_nsamp, nline, nsamp, demname_filled, 
                      offset_filled)
        write_shelve(database_fname + ":job_%d" % job_index, job)
        job_index += 1
number_job = job_index

# Create a script that can submit these jobs
log_dir = args.output_base + "_log"
shelve_job_run = subprocess.check_output(["which", "shelve_job_run"]).replace("\n", "")
gdal_to_erdas = subprocess.check_output(["which", "gdal_to_erdas"]).replace("\n", "")
pythonpath = os.environ["PYTHONPATH"]

makedirs_p(log_dir)

job_script = '''#!/bin/bash
if [[ $# -lt 1 ]]; then
   echo "Usage: $0 <arguments to qsub>"
   echo ""
   echo 'At a minimum, you need to supply the queue to use, e.g., "-q long"'
   exit 1
fi
export SHELVE_DATABASE_FILE="{database_fname}"

jid=`qsub -d {work_dir} -V -j oe -o {log_dir}/qsub.log -N dem_generate -t 0-{max_array_num} {shelve_job_run} $*`
# Add in ERDAS generation here
qsub -d {work_dir} -V -j oe -o {log_dir}/qsub.log -N to_erdas -W depend=afterokarray:$jid {output_base}.to_erdas $*
'''

with open(args.output_base + ".job_script", 'w') as f:
    f.write(job_script.format\
                (database_fname=database_fname,
                 log_dir=log_dir,
                 shelve_job_run=shelve_job_run,
                 max_array_num = number_job - 1,
                 output_base = args.output_base,
                 work_dir = os.path.abspath(os.path.dirname(database_fname)),
                 ))
os.chmod(args.output_base + ".job_script", 0755)

to_erdas_script = '''#!/bin/bash
export PYTHONPATH={pythonpath}
{gdal_to_erdas} {output_base}.img {output_base}_erdas.img
{gdal_to_erdas} {output_base}_filled.img {output_base}_filled_erdas.img
'''

with open(args.output_base + ".to_erdas", 'w') as f:
    f.write(to_erdas_script.format(output_base = args.output_base,
                                   gdal_to_erdas = gdal_to_erdas,
                                   pythonpath = pythonpath))
os.chmod(args.output_base + ".to_erdas", 0755)
